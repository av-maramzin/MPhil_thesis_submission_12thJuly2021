\chapter{Background and Motivation}
\label{background}
\label{background}
\quad \textit{Parallel computing} and \textit{software parallelization} are vast, overlapping, and complimentary computer science areas with a rich history dating back to the 1950s. With advances in the semiconductor industry, the topics have left the niche of high-end scientific supercomputers and spread to a much wider area spanning across all consumer electronic devices and have become of major importance.\newline\null
% Parallel computing and software parallelization are vast, complementary, and overlapping areas omnipresent across the whole spectrum of hardware. The topics of major importance.
\quad Nowadays, parallelism is pervasive. Parallel hardware is omnipresent across the whole wide spectrum of various computing systems. To exploit all available hardware capabilities software has to be parallel as well. And thus, every computer scientist and software developer would benefit from having an insight into the area. Nonetheless, the topics are extremely complex, require a serious time investment and a great deal of knowledge in various subdomains. It is not realistic to expect an average programmer to possess such deep expertise. For that reason, we propose a solution aimed at alleviating the challenging task of manual software parallelization. Our solution consists of two components. We describe them in Chapters \ref{assistant} and \ref{frameworks}.\newline\null
% Background chapter as the point of basic material accumulation. Based on LLNL tutorials!
\quad In this chapter, we stress the importance of software parallelization, highlight its challenges, describe the parallel software engineering process, and finally lay the ground for our proposed solutions from Chapters \ref{assistant} and \ref{frameworks}. \textbf{The major ideas leading towards the solutions from Chapters \ref{assistant} and \ref{frameworks} are highlighted with boldface text.} We express our special gratitude to Lawrence Livermore National Laboratory (LLNL) \cite{llnl_computing} for their great parallel computing tutorials. We heavily relied on those to prepare the background material.\newline\null
% The story line:
%
% Parallel computing importance
% Challenges of software parallelization:
%     automatic
%     manual
%     machine learning based
%     data-centric parallelization problem
% 
%
\quad The background chapter is structured as follows. Section \ref{background_importance} stresses the importance of parallel computing and software parallelization in the modern world. There are numerous software parallelization methods and techniques that address the problem, but all of them run into specific challenges and limitations. Section \ref{background_challenges} highlights the major problems of various software parallelization methods. First, it presents the challenges of manual and then automatic software parallelization techniques. There have been various experiments and works applying machine learning (ML) based methods to the field of compilers \cite{ml-oboyle} and software parallelization in particular \cite{fried_ea:2013:icmla}. The challenges of manual and automatic parallelization combined with the idea of using machine learning based methods lay the ground for our program loop parallelization assistant solution \cite{assistant-aiseps}. We describe our assistant solution in Chapter \ref{assistant}.\newline\null
\quad Section \ref{background_dcp} discusses the problem of data structure choice and how it affects software parallelization. Unfortunately, there are no universal automatic solutions to this problem at the moment. Data structures are often inseparable from the algorithms they support. Our computational frameworks build on that fact by defining the blend of data structures and algorithms. We propose our solution to the problem in Chapter \ref{frameworks}. It is very important to compare the concept of computational frameworks to a well-established concept of parallelism patterns (algorithmic skeletons). Section \ref{background_frameworks_vs_skeletons} presents this comparison. Furthermore, modern software design and engineering tasks are extremely rich and complex topics. Of course, that is true of parallel software engineering as well. In Sections \ref{background_programming_paradigms} and \ref{background_oop_design} we talk about imperative, functional and object-oriented programming paradigms, as well as various OOP software design patterns. In Section \ref{background_frameworks_design} we explain how our computational frameworks take the best from these principles.
\section{The importance of parallel computing}
\label{background_importance}
\quad Parallelism is pervasive and the future of computing is parallel. Many factors stress the importance of parallelism in the modern computing world.
\begin{description}[style=unboxed,leftmargin=0cm]
\itemsep0em
\item[Abundance of natural parallelism] The field of High-Performance Computing (HPC) has traditionally been concerned with scientific modeling and simulation of various natural phenomena (climate change, fluid flows, etc.). Physical systems consist of numerous, often independent parts. Moreover, these problems are often expressed through common parallelizable mathematical models: parallel Gauss \cite{SHANG20091369} and Conjugate Gradient methods \cite{10.1007/978-3-642-14390-8_14}, \cite{HELFENSTEIN20123584} for solving linear equation systems, parallel matrix inversion \cite{SHARMA201331}, parallelizable optimizations for specific cases (like sparse matrices \cite{CHEN201849}, \cite{DEVECI201833}), etc. When we compile a highly parallel algorithm to a serial sequence of CPU instructions or process a huge dataset with independent parts sequentially, we artificially constrain a vastly parallel computation to a serial one.
\item[Semiconductor technology advances and power limits] With advances in transistor density, it became feasible to design more complicated CPUs. Initially, the trend went towards more complex microarchitecture with deeper pipelines, but running into power limits the industry design shifted towards multi-core CPUs and multiprocessor systems. To exploit such systems fully, software must mirror the trend and become parallel as well.
\item[Domain inherent parallelism and specialized computations] The areas like computer graphics for instance have a lot of problems that can be processed in a Single Instruction Multiple Data (SIMD) fashion. That naturally led to the emergence of specialized co-processors like GPUs making hardware systems more complex and heterogeneous \cite{cpu-heterogenuity}.
\end{description}
\quad To fully exploit all capabilities provided by modern high-performance computing systems, software has to be mapped onto the parallel hardware i.e parallelized.

\section{Challenges in software parallelization}
\label{background_challenges}
\quad The problem of software parallelization is extremely complex and multi-faceted. There are various approaches to the problem, but all of them have their pros and cons. Although the process of software parallelization has characteristically been a very \textit{manual} task, which is time-consuming and error-prone, there are also \textit{automatic} and \textit{machine learning based} techniques. In this section, we highlight the inherent problems of all these approaches. The solution we propose grows on these challenges.
\subsection{Manual parallelization challenges}
\label{background_challenges_manual}
\quad Parallel software development has characteristically been a very manual process. Like any software development process, it consists of several stages. The major problems are described below.
\begin{description}[style=unboxed,leftmargin=0cm]
\itemsep0em
\item[Problem understanding and partitioning] As the best software engineering practices dictate, before diving into software development one needs to thoroughly understand the problem and decide on the requirements and restrictions the final piece of software must meet. The whole algorithm and software architecture might change with the decision of developing a parallel software version instead of a serial one. If one starts from an already implemented serial software version, the parallelization might be even more difficult to do. Source code comprehension is a hard task. The algorithm chosen for a serial version might be completely unsuitable for a parallel implementation. The problem must be partitioned into relatively independent chunks of work to be processed in parallel. The partitioning can be done in multiple ways and a programmer needs to choose one (data set decomposition, functional decomposition, or a hybrid of the two).
\item[Communications and synchronization] Very often the parts of the problem are not completely independent and require an exchange of information. Designing the way that exchange is going to work is a complex task. Almost always communication results in overhead. Sending the data over a congested network or waiting on a synchronization barrier slows the program down. The slowdown might even diminish all performance benefits obtained from parallelization.
\item[Implementation and data dependencies] When the problem partitioning is done, all communication and synchronization points are determined and the high level parallel algorithm is designed, a programmer might start the actual implementation. Here a programmer will run into other types of problems. Consider two functionally equivalent code samples below.\newline\null
\begin{minipage}[t]{0.50\linewidth}
\begin{lstlisting}[caption={\raggedright Non-parallelizable loop with planted loop-carried data dependence.},label={lst:code_sample_data_dependence},language=C]
for (int i=1; i<n; i++) {
  a[i]=a[i-1]+1;
}
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.50\linewidth}
\begin{lstlisting}[caption={\raggedright Parallelizable loop free of any data dependencies.}, label={lst:code_sample_no_data_dependence},language=C]
for (int i=0; i<n; i++) {
  a[i]=a[0]+i;
}
\end{lstlisting}
\end{minipage}

The actual shape of the code can break its parallelization by introducing fake (not required by the algorithm) dependencies an average state-of-the-art compiler cannot tackle.
\item[Performance analysis and tuning] One needs to know where the program's hotspots are. Hotspots are the places where most of the real work is done. The majority of programs spend most of the CPU time in a few places. The task of a programmer is to find those places and concentrate all parallelization and optimization efforts there. Finding hotspots might be difficult before the programmer has the whole program implemented. Modern hardware architectures have a multi-level memory hierarchy, memory data prefetchers, TLBs, out-of-order execution, etc. It might be surprising how the actual program execution performance differs from the one inferred from the algorithm. Profilers and other analysis tools can help here \cite{perf-tool}.
\end{description}
\quad Finally, all the above challenges are interrelated and very often depend on each other. The parallel software development process can go iteratively with numerous dead ends and redesign efforts. With a long research history into the topic, all these problems are still actual to this day.
\subsection{Limitations of automatic techniques}
\label{background_challenges_automatic}
\quad Given the difficulty of manual software parallelization, there have been numerous ongoing efforts into various automatic parallelization tools. The vast amount of legacy sequential software developed over the last few decades further exacerbates the need. Automatic parallelization refers to converting sequential code into multi-threaded and/or vectorized code in order to use multiple processors simultaneously in a shared-memory multiprocessor (SMP) machine.\newline\null
\quad There are various tools available to a programmer for automating the task of software parallelization. We present an overview of the field in the Section \ref{related_work_autopar}. Parallelizing compilers are the most widely used nowadays. Automatic parallelization tools can be classified into two types:
\begin{description}[style=unboxed,leftmargin=0cm,noitemsep]
\itemsep0em
\item[Fully Automatic] The compiler analyzes the source code and identifies opportunities for parallelization. The analysis includes identifying inhibitors to parallelism and possibly a cost weighting on whether or not the parallelization would improve performance \cite{Kennedy:2001:OCM:502981}. Loops are the most frequent target for automatic parallelization \cite{Bacon:1994:CTH:197405.197406}.
\item[Programmer Directed] Using compiler directives or possibly compiler flags, a programmer explicitly tells the compiler how to parallelize the code. These directives and flags may be also used in conjunction with some degree of automatic parallelization. The most common compiler-generated parallelization is done using on-node shared memory and threads (such as OpenMP \cite{Dagum:1998:OIA:615255.615542}).
\end{description}
\quad If one starts with an existing serial code and has the time or budget constraints, then automatic parallelization may be the answer. However, several important caveats apply to automatic parallelization.
\begin{description}[style=unboxed,leftmargin=0cm]
\itemsep0em
\item[Performance] Performance may degrade.
\item[Limitations] Limited to a subset (mostly loops) of code.
\item[Effectiveness] May not parallelize the code, i.e. it may be "over-conservative" or the code is too complex.
\end{description}
\quad To clearly illustrate the problems an automatic software parallelization faces we conducted several experiments with the suite of NASA Parallel Benchmarks (NPB) \cite{nasa-parallel-benchmarks}. These benchmarks target the performance evaluation of highly parallel supercomputers. Consequently, the suite has a great amount of inherent parallelism and is supposed to be easily parallelizable. NPB benchmarks do not provide the exact implementation, they rather specify what should be computed and how. We used Seoul National University's (SNU) implementation \cite{snu-npb-benchmarks} of NPB benchmarks. SNU NPB implementation comes in two versions: sequential legacy C implementation and the one parallelized with OpenMP pragmas. The main data structure used everywhere in the suite is a simple flat array. Numerous loop nests operate over arrays and compute simple reductions.\newline\null
\quad For these experiments we used a desktop Ubuntu 18.04 machine with installed Intel C/C++ Compiler (ICC) 18.0 and measured the running time of benchmarks with the help of UNIX time() utility. To minimize the errors, we ran experiments several times and took the mean average. The machine has 4 Intel Core i5-6500 CPUs with 3.20 GHz frequency and vectorization support up to AVX2. The RAM is 16Gb.\newline\null
\quad The first experiment we conducted was aimed at assessing the effectiveness of the state-of-the-art automatic parallelization. The experiment is platform agnostic as long as the target supports vector instructions and parallel primitives and can be reproduced on any such platform. We took the Intel C/C++ Compiler (ICC), configured it for the most aggressive parallelization (\textit{-par-threshold0}), i.e parallelize all parallelizable code independent of its potential cost weighted profitability. Also, we configured ICC to do all enabling loop transformations (\textit{-O3} flag) before the actual parallelization (\textit{-parallel} flag) and vectorization (\textit{-vector} flag). In other words, the experiment measures the maximum parallelization coverage the best state-of-the-art compiler can achieve on embarrassingly parallel problems, which still represent the real-world code. Our results show a significant potential for improvement. Among all 1415 SNU NPB loops, 980 are truly parallelizable, but the ICC compiler manages to find only 812 parallelizable cases and misses 168 loops. Table \ref{tab:icc_missed} shows the classification we conducted by manually examining the ICC parallelization reports as well as looking at the source code of the benchmarks. The biggest problems are statically unknown pointers, which might potentially overlap at the running time, as well as other statically unresolvable dependencies. There are some unrecognized reductions as well as loops that can be parallelized with prior array privatization and function inlining. In 60 cases ICC could not parallelize the loop due to pointers and alias analysis conservativeness. Static dependencies are largely indirect array references.
\begin{table}
  \begin{minipage}{\pagewidth}
  \begin{center}
    \begin{tabu}{M{3.0cm}M{1.0cm}M{3.0cm}M{1.0cm}M{3.0cm}M{1.0cm}}
      \hline
      \rowfont{\bfseries}
      reason & num & reason & num & reason & num\\\hline
      \textbf{unrecognised reduction} & 18 & \textbf{array privatization} & 7 & \textbf{AA conservativeness} & 60\\\hline
      \textbf{unknown iteration number} & 7 & \textbf{static dependencies} & 46 & \textbf{too complex} & 22\\\hline
      \textbf{uninlined calls} & 4 & \textbf{other} & 4 & \textbf{total} & 168\\\hline
    \end{tabu}
  \end{center}
  \end{minipage}
  \caption{Classification of loops missed by Intel Compiler for various reasons.}
  \label{tab:icc_missed}
\end{table}\newline\null
\quad While parallelization coverage is important it is not the primary goal. A parallelized loop might not make a significant contribution to the total running time of the application. We should strive to parallelize those loops, which are on the critical paths and hot spots. And finally, only the running time is the ultimate parallelization performance measure. Given that, we conducted a further experiment. We used the same machine and compiled SNU NPB benchmarks with different sets of ICC automatic parallelization options. Figure \ref{fig:benchmarks_runtime} illustrates the running times of resulting codes. Bars marked as serial (s) show running times of original legacy C sequential versions. Bars marked as omp (o) show running times of an expertly manually parallelized versions. Other bars show running times of versions produced with various combinations of ICC compiler options (vectorize, parallelize or do both), as well as complete cases of parallel OpenMP versions, which have been additionally parallelized and vectorized by the ICC compiler. One can see that the best performance is still attributed to benchmark versions, which have been expertly parallelizad by their developers. Vectorization and parallelization help parallel versions a bit, but not that significantly and the profits can be neglected altogether. When we automatically vectorize serial versions we get a little improvement, but when we try to automatically parallelize them we get striking slowdowns on some benchmarks. Overall, automatic vectorization gives us a tiny 1.1x running time improvement in the geometric mean compared to 1.73x of manual parallelization. And the automatic parallelization results into 0.79x slowdown in the geometric mean across all the benchmarks.\newline\null
\textbf{\quad Our machine learning based loop parallelization assistant \cite{assistant-aiseps} we propose in Chapter \ref{assistant} extends parallelism recognition capabilities of the Intel C/C++ Compiler (ICC) by learning the loop parallelizability property and making predictions regarding it with an acceptable false positives rate. These predictions cover most of the cases missed by ICC. Moreover, our assistant helps to reach the best possible manual expert performance faster by guiding the programmer towards to the most fruitful code segments to parallelize.}
\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{images/benchmark_runtime.pdf}
\caption{The running time of various NPB benchmarks versions.}
\label{fig:benchmarks_runtime}
\end{figure}
\subsection{Limits of machine learning based methods}
\label{background_challenges_ml}
\quad Correctness is the most important property of the code. Although important, the running time or any other type of code performance characteristic is not always vitally critical. As all machine learning based techniques have always been characterized by their inherent and ineradicable errors \cite{James:2013:ISL:2517747}, the field of compilers and especially the problem of automatic parallelization have never been the primary targets for these methods. Section \ref{related_work_ml} discusses the relevant work in more details.\newline\null
\quad The application of machine learning based methods to the problem of software parallelization has not yet found a widespread practical utility. Mispredictions regarding the code parallelizability can lead to a broken dependency property and thus incorrect program execution. Nonetheless, there have already been works on predicting loop parallelizability, like the approach of Fried \emph{et al.}~\cite{fried_ea:2013:icmla}. Fried \emph{et al.} train a supervised learning algorithm on code hand-annotated with OpenMP parallelization directives to create a loop parallelizability predictor. These directives approximate the parallelization that might be produced by a human expert. Fried \emph{et al.}~focus on the comparative performance of different ML algorithms and studies the predictive performance that can be achieved on the problem and does not produce any practical application.\newline\null
\textbf{\quad In Chapter \ref{assistant} we describe a practical ML-based loop parallelization assistant \cite{assistant-aiseps}. Contrary to Fried \emph{et al.}~\cite{fried_ea:2013:icmla} it uses static source code features. Moreover, we use a richer training set, which is not limited to expert OpenMP pragmas only, but additionally takes the information from the Intel C/C++ Compiler. While Fried \emph{et al.}~focus on the comparative performance of different ML algorithms and study the possibility to learn loop parallelizability property, we do the same, but
additionally, we contribute a practical assistant capable of ranking loop candidates in their order of merit.}
\section{Data-Centric Parallelization (DCP) problem}
\label{background_dcp}
\quad The problem of data-centric parallelization (DCP) is the central motivation for the concept of computational frameworks we propose in Chapter \ref{frameworks}. As it has already been stated the problem of software parallelization is multifaceted. There is a vast range of lower level technical issues, which can turn a perfectly parallelizable at a higher level computation into a non-parallelizable implementation. For example, in Section \ref{background_challenges_automatic} we showed that the main reasons of Intel Compiler failures on SNU NPB benchmarks are alias analysis conservativeness, uninlined function calls and statically unresolvable dependencies. These reasons do not close the set of all possible parallel algorithm implementation failures. Listings \ref{lst:array} and \ref{lst:list} clearly illustrate a yet another potential implementation failure.\newline\null
\begin{minipage}[t]{0.5\linewidth}
\begin{lstlisting}[caption={\raggedright Parallelizable loop operating on what is clear to compiler a \textbf{linear array}.},label={lst:array},language=C]
int a[1024];
for (int i=0; i<1024; i++) {
  a[i]=a[i]+1;
}
\end{lstlisting}
\end{minipage}
%
\begin{minipage}[t]{0.5\linewidth}
\begin{lstlisting}[caption={\raggedright Non-parallelizable loop operating on what programmer knows is a \textbf{linked-list}.},label={lst:list},language=C]
struct Node* nptr;
for (p=nptr; p!=NULL; p=p->next) {
  p->value+=1;
}
\end{lstlisting}
\end{minipage}

\quad The above code snippets present two alternative implementations of the same simple and embarrassingly parallel computation. We increment all sequence elements by one. Listing \ref{lst:array} implements the sequence with a regular array linearly laid out in the memory. Listing \ref{lst:list} chooses a linked list as an implementing data structure. While in an array-based implementation the compiler knows all element addresses statically and can generate parallel code in advance in the linked list based implementation element addresses can be resolved only dynamically, which leads to a source code non-parallelizability. Furthermore, compiler does not know what kind of a pointer-based data structure we iterate over.\newline\null
\quad The data-centric parallelization problem is how to automatically recognize what kind of a data structure is used in the code: is it a tree, a linked-list, a directed acyclic graph? The DCP problem is not solved yet. Automatic methods are limited in their recognition capabilities to relatively simple code bases such as libraries of well-known data structures. Automatic transformation is even harder. The most successful methods rely on the dynamic analysis of memory graphs. Static techniques such as shape analysis are undecidable and highly conservative and might not finish in a reasonable time for the real software projects. The Section \ref{related_work_dcp} gives a comprehensive literature review on the topic.
\section{Imperative and functional programming}
\label{background_programming_paradigms}
\quad Programming languages can be classified by different \textit{programming paradigms} they support. Among the most general classifications are \textit{imperative} and \textit{declarative} programming languages.\newline\null
\quad Imperative programs are written in a form of instruction sequences, which read and write the \textit{state} of a program. The concept of state is the main characteristic of the imperative programming paradigm. Instruction sequences can be structured in various ways. In \textit{procedural programming} paradigm instructions are grouped inside procedures and functions. In \textit{object-oriented programming (OOP)} paradigm instructions are grouped with the data they operate on inside objects of various types or classes. Programs are built either out of various procedures calling each other and exchanging the data or on the interaction of objects of various types. Imperative programs specify the exact sequence of steps to take in order to compute the final result.\newline\null
\quad Declarative programs do not specify the exact sequence of steps and state updates a program needs to do to get the desired result. Declarative programs declare the properties of the desired result. The properties can be specified as a set of constraints like in \textit{constraint programming} or a set of linear inequalities like in \textit{linear programming}. \textit{Functional} programming is another subtype of declarative programming. In functional programming, the final result is specified as a sequence of stateless function evaluations, which form a tree of expressions. Among the most common constituents are functions like \textit{map}, \textit{reduce}, \textit{fold}, etc. Functions can be passed as arguments and returned from other functions ultimately composing bigger programs.\newline\null
\quad Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming that treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.\newline\null
\quad There are no universally optimal programming paradigms and languages. Some languages are more convenient and suitable for one sort of problem, some languages are better at tackling other problems. For example, functional languages are more convenient in addressing certain domains such as R for statistics and financial analysis. Imperative languages are certainly better for simulations and other state-based scientific computations. For that reason, major languages are often multi-paradigm to cover a potentially larger set of problems. Largely imperative C++ language included support for functional programming with its newer standards starting from C++11.\newline\null
\section{OOP and software design patterns}
\label{background_oop_design}
\subsection{Object-Oriented Programming (OOP)}
\label{background_oop}
\quad \textit{Object-oriented programming (OOP)} is arguably the most widely used programming paradigm nowadays. It is supported by almost all major programming languages. At the very essence, in OOP computer programs are designed by making them out of objects that interact with one another. Object interactions are very close to the human level of reasoning and logic and thus the paradigm fits quite naturally to a human developer.\newline\null
\quad Objects are instances of different types or classes in OOP terminology. Classes are object specifications. They specify the data objects contain (like an integer \textit{age} field for an object of class \textit{Person}) and the methods used to operate on the data. Classes define the public part of objects as well as their internal implementation. Object-oriented (OO) languages provide a rich set of facilities and features to build programs.\newline\null
\quad \textit{Encapsulation} is used for protection against object misuse and unintended outside interference: data and methods concerned with internal workings are declared \textit{private}, while those designed to form an outward appearance are declared \textit{public}. This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code. It also eases debugging by better localizing functionality and thus possible bugs.\newline\null
\quad \textit{Dynamic dispatch} is the responsibility of the object, not any external code, to select the procedure to execute in response to a method call, typically by looking up the method at run time in a table associated with the object. This feature allows a programmer to write general code, which works with abstract interface methods and leaves the exact method resolution to be made during the running time of a program.\newline\null
\quad Dynamic dispatch is closely related to the technique of \textit{inheritance}. Inheritance allows classes to be arranged in a hierarchy that represents "is-a-type-of" relationships. Inheritance can be of two types: interface and implementation inheritance. The first one allows a parent class to require its descendants to stick to the same interface. A common interface allows the objects of different classes from the same hierarchy to be operated on by the same type agnostic code. The latter is called a \textit{polymorphism}. The user code can be more concise and abstract. The call of the same method on the parent class or one of its descendants can result in a varying behaviour.\newline\null
\quad Features that are described above do not close the set of all available OOP techniques and mechanisms. These are just the main features we rely on in our project of computational frameworks and are the most important to know.
\subsection{Software design patterns}
\label{background_design}
\quad The presence of all the above features makes OOP languages extremely rich with various facilities. That creates a vast design space for software architects and turns the OOP software design into an art. \textit{Software design patterns} \cite{gang-of-four} are reusable solutions to common design problems in OOP. Design patterns have been well tested and are proven to be the most reliable and elegant solutions for the design problems they target.\newline\null
\quad Design patterns live at an intermediate level between the exact algorithm and a programming paradigm and are language agnostic. Design patterns specify and document solutions to common design problems. These solutions consist of a set of classes, objects, and the ways they interact with one another. It is agreed to classify patterns into 4 distinct categories: creational, structural, behavioral, and concurrency patterns. Here we are going to mention only those patterns, which are the most relevant to our project of computational frameworks.\newline\null
\quad The \textit{command} pattern is a behavioral pattern, where a command object is used to encapsulate all the information needed to perform an action or trigger some event at a later time. The other participants are the receiver, invoker, and client. The central ideas of this design pattern closely mirror the semantics of first-class functions and higher-order functions in functional programming languages. Specifically, the invoker object is a higher-order function of which the command object is a first-class argument.\newline\null
\quad One can see the command pattern used in combination with the \textit{chain of responsibility pattern}, which is a design pattern consisting of a source of command objects and a series of processing objects. Each processing object contains logic that defines the types of command objects that it can handle; the rest are passed to the next processing object in the chain. In a variation of the standard chain of responsibility model, some handlers may act as dispatchers, capable of sending commands out in a variety of directions, forming a \textit{tree of responsibility}. The chain of responsibility pattern promotes the idea of \textit{loose coupling}, where the system consists of multiple components, which know little or nothing about the definitions of other components.\newline\null
\quad Another very relevant pattern is a \textit{template method} pattern. The template method is a method in a superclass, usually an abstract superclass, and defines the skeleton of an operation in terms of a number of high-level steps. These steps are themselves implemented by additional helper methods in the same class as the template method. The helper methods may be either abstract methods, for which case subclasses are required to provide concrete implementations, or hook methods, which have empty bodies in the superclass. Subclasses can (but are not required to) customize the operation by overriding the hook methods. The template method intends to define the overall structure of the operation while allowing subclasses to refine, or redefine certain steps.\newline\null
\quad The \textit{visitor} pattern is a way of separating an algorithm from an object structure on which it operates. A practical result of this separation is the ability to add new operations to existing object structures without modifying the structures. In essence, the visitor allows adding new virtual functions to a family of classes, without modifying the classes. Instead, a visitor class is created that implements all of the appropriate specializations of the virtual function. The visitor takes the instance reference as input and implements the goal through double dispatch.\newline\null
\quad Software design patterns can be more down to the language like the \textit{curiously recurring template pattern (CRTP)}, which is an idiom in C++ where class X derives from a class template instantiation using X itself as template argument.\newline\null
\quad \textit{Software design patterns} specify and document the best practices to solve various commonly reoccurring problems. In the end, it is the experience, mastery, and ingenuity of a programmer which determine the final software design.
\section{Algorithmic skeletons and Parallel design patterns}
\label{background_skeletons}
\quad An overview paper \cite{skeletons-overview} presents the concepts of \textit{algorithmic skeletons} (or \textit{parallelism patterns}) and \textit{parallel design patterns} and tracks the way how these concepts have permeated the mainstream parallel programming community. We heavily relied on this work to prepare the material below.\newline\null
\quad In the last two decades, the general-purpose computing industry has made a massive shift towards parallelism. The number of various parallel hardware architectures available on the market has substantially increased. The change is not just quantitative: the number of parallel CPUs and CPU cores has increased, as well as the diversity and heterogeneity of parallel computing systems \cite{welcome-to-the-jungle}. Modern systems feature various accelerators such as GPUs integrated with networked parallel CPU clusters into rather heterogeneous and complex computing environments.\newline\null
\quad The already difficult task of parallel programming (see Section \ref{background_challenges_manual}) has become even more challenging, and the need for adequate programming models and frameworks to ease the job of parallel application programmers has become acute. Hardware architectures become pervasively parallel and grow in their complexity. There is an ongoing research effort that addresses the ease of programmability of the next-generation computing systems. Various structured parallel programming models have been introduced to tackle the problem.\newline\null
\quad Analogous to structured sequential programming, where \textit{if-then-else}, \textit{switch statement}, etc. have replaced unstructured \textit{goto}-based programming style, parallel structured programming models ban certain programming practices. Concurrency can only be expressed and orchestrated through well-established parallel "forms" and structured compositions of those. Structured parallel programming fosters not only more maintainable code but also makes it more parameterizable and predictable in terms of performance behavior.\newline\null
\quad Relatively low-level standards like POSIX threads \cite{posix_specs} no longer meet all emerged requirements to harness underlying hardware complexity and diversity and at the same time provide a convenient abstraction to application programmers. A number of further "de facto standards" like OpenMP (Open Multi-Processing) \cite{openmp_specs} for shared memory architectures, CUDA (Compute Unified Device Architecture) \cite{cuda} and OpenCL (Open Computing Language) \cite{opencl} for general-purpose GPUs, MPI (Message Passing Interface) \cite{mpi} for distributed systems have been designed and introduced into the industry. Furthermore, the key industry players proposed other higher-level frameworks such as Intel TBB (Threading Building Blocks) \cite{intel-tbb} or Microsoft PPL (Parallel Patterns Library) \cite{microsoft-ppl}. Arguably, these models encompass the key results from academic research in the area of parallelism patterns.
\begin{description}[style=unboxed,leftmargin=0cm,noitemsep]
\itemsep0em
\item[Algorithmic skeletons or Parallelism Patterns] The algorithmic skeleton (parallelism pattern) concept was introduced into academia by Murray Cole in his PhD thesis in the late â€™80s \cite{mcole-thesis}. The algorithmic skeleton concept is intended to address the difficulty of programming parallel hardware architectures by separating two concerns: expressing application's parallelism through a composition of well-established and well-known primitives or algorithmic skeletons and designing and developing libraries of these primitives on a system programming side. Since the inception of the concept, a number of various algorithmic skeletons have been proposed. Among the most well-known patterns are \textit{map}, \textit{pipeline}, \textit{reduce}, \textit{scan}, \textit{stencil}, \textit{divide and conquer}, \textit{task farm}, etc. More complex patterns can be composed of the basic ones. Algorithmic skeletons are higher than other parallel programming models and thus more human-friendly and portable. A more detailed description can be found in the book on structured parallel programming \cite{mccool-patterns}. For example, a \textit{pipeline} algorithmic skeleton can be defined like this:\newline\null
\textbf{Definition} A \textit{pipeline} is a chain of parallel processing entities arranged in a way so the output of each entity is the input to the next one. Suppose the pipeline consists of $n$ stages described by functions $f_{1},...,f_{n}$. Then, for each data $x$ in the input stream, the functions are applied consecutively to produce the final output $f_{n}(...(f_{2}(f_{1}(x))))$. When the stage $f_{j}$ is done processing its input $x_{i}$, it can start to process the next input element $x_{i+1}$ in the data stream. The next stage $f_{j+1}$, in turn, starts processing $f_{j}(x_{i})$ data now in parallel. There are must be no race conditions between stages.\newline\null
\quad The above description presents a very general high-level specification that frames a parallel computation and leaves some space for creating various versions of the definition. An example of a more constraining definition can be found in the work \cite{skeletons-static} (see Section \ref{related_work_as_and_pp}), which requires functions $f_{1},...,f_{n}$ to be pure, i.e have no side-effects. Although, the violation of this requirement does not break pipeline parallelizability, as long as there are no race conditions between the stages.
\item[Parallel design patterns] The concept of software design patterns (see Section \ref{background_oop_design}) has migrated to the world of parallel programming in '00s. A design pattern is a "recipe", that addresses some specific computational scenario. This recipe describes a problem together with the best-known solution. Parallel design patterns are described in the space of 4 dimensions: finding concurrency (i.e what parallelism can we exploit?), algorithm design (finding suitable algorithms), implementation structures, and execution mechanisms (multi-core CPUs, vector instructions, etc.). The table \ref{tab:pipeline_design_pattern} below illustrates an example of \textit{pipeline} parallel design pattern.
\begin{table*}[!ht]{\linewidth}
  \tabulinesep=2pt
  \begin{minipage}{\linewidth}
  \begin{center}
    \begin{tabu}{M{3cm}M{11.0cm}}
      \hline
      \rowfont{\bfseries}
      Name & Pipeline\\\hline
      Problem & Computation is organized in stages, over a large number of independent data sets\\
      Examples & Assembly line, CPU instruction fetch/decode/execute cycle, video frame processing with multiple filters, etc.\\
      Features & Parallel stage computation, input/output ordering, etc.\\
      Implementation & Set up a chain of parallel activities, each one processing a stage, receiving input data from the previous stage, and delivering results to the next stage through proper
      single-producer single-consumer message queues.\\
      Sample implementation & A sketch of MPI-based code computing a pipeline\\\hline
      \end{tabu}
  \end{center}
  \caption{Sketch of sample pipeline parallel pattern specification (the whole pattern may easily take tens of pages to be
  properly described \cite{mccool-patterns}).}
  \label{tab:pipeline_design_pattern}
  \end{minipage}
\end{table*}%
\end{description}
\quad Parallel design patterns specify computations on a very high level, leaving the whole software design, business logic implementation, and system-level parallelization to an application programmer. While, algorithmic skeletons usually come as libraries that implement low-level system side of parallelization behind a convenient API, alleviating the task for an application programmer.\newline\null
\quad Algorithmic skeletons and parallel design patterns themselves are largely well-established and are not at the cutting edge of the research effort. At the same time, the fifty years of parallel programming have generated a substantial amount of parallel legacy code using lower-level, more hardware-specific, and, hence, less portable, less maintainable code. There is a range of ongoing research efforts to automatically identify various parallelism patterns in a legacy code and substitute them with their rejuvenated modern counterparts. Unfortunately, refactoring tools are still premature and are not fully adopted by development centers yet. Most of them are human-supervised, where a developer is responsible either for approving or doing the final manual transformation of the code. Although these tools relieve the burden of the source-to-source transformation, this process remains semi-automatic. The Section \ref{related_work_as_and_pp} provides a literature review on the subject.\newline\null
\section{Computational frameworks}
\label{background_frameworks_vs_skeletons}
\quad The idea of computational frameworks we propose in this thesis (see Chapter \ref{frameworks}) requires some clarification regarding its difference from algorithmic skeletons and its relationship with software design patterns and programming paradigms. The detailed discussion of computational frameworks is presented in Chapter \ref{frameworks}. Here we introduce some clarifications to set a reader in advance.  
\subsection{A debate: computational frameworks vs. algorithmic skeletons}
\label{background_frameworks_vs_skeletons}
\quad The difference between computational frameworks and algorithmic skeletons is very subtle. Let's start with considering \textit{map} and \textit{reduce} algorithmic skeletons. These skeletons specify a higher level computation to be applied to a set of elements. The elements can be arranged into an arbitrary graph, a sequence, a mesh, etc. These algorithmic skeletons are data structure agnostic as long as a certain computation can be applied to individual elements and produce a meaningful result. On the implementation side, data structures must provide proper iterators and conform to a required interface. A stencil algorithmic skeleton contains a notion of an underlying data structure. A stencil specifies how to combine adjacent elements of a regular data structure in higher-level parallel computation. A stencil could be computed on a 3D lattice, 2D mesh, or on a flat array. Although a stencil requires certain properties of an underlying data structure, it is not very specific.\newline\null
\quad The difference between well-established algorithmic skeletons and computational frameworks is best explained on a \textit{fractal} (see Section \ref{frameworks_fractal}), which is a form of reduction done over a tree data structure. The tree can be implemented as an array of pointers to leaves or a binary heap laid out on a flat array. Fractal is implementation agnostic in that respect, as it only requires the underlying data structure to be a tree and computation to follow a specified higher-level order. We cannot compute a fractal over a sequence or an arbitrary graph abstract data structure. Of course, a fractal can be implemented with a set of reductions, but it will no longer be a unified primitive.\newline\null
\quad While algorithmic skeletons are a higher-level concept that is either agnostic to an underlying data structure or implies some limitations while still allowing data structure variance within a certain space. The concept of computational frameworks is bound to a certain data structure over which it operates. In other words, it has both an algorithmic as well as data structure components.\newline\null
\quad In addition to fractal, the prototype library we implement to assess the concept also includes \textit{map} and \textit{reduce} algorithmic skeletons. This demonstrates the proximity of the two concepts and their compatibility. In our work, \textit{fold} is also viewed as a computational framework and not an algorithmic skeleton, since it requires an underlying data structure to be a sequence. An \textit{fold} algorithmic skeleton would only require of the underlying data structure to be recursive and, hence, is more higher-level, general and contains only an algorithmic component with minimal restrictions on the data structure. Both concepts are compatible.
\subsection{Computational frameworks design}
\label{background_frameworks_design}
\quad In Section \ref{frameworks_library_design} we describe the design of our prototype library \cite{frameworks-repo}. The concept of computational frameworks and the prototype library we implement lie within the gap between functional and imperative programming paradigms: some problems contain computations, which are better expressed with standard functional concepts (like \textit{maps}, \textit{folds}, \textit{reductions}, etc.), but at the same time require some state keeping. An example can be some scientific simulation. These problems lie at the boundary of functional and imperative programming. Our library provides a functional style interface and is developed in an object-oriented fashion.\newline\null
\quad The library implementing the concept of computational frameworks has been designed with some software design patterns in mind. While computational frameworks specify a higher-order algorithm, they leave a space for customization. The latter is implemented with the help of a template method design pattern, which also specifies the main computation and leaves some lower-level steps to be defined by a user. One can see a similarity between the chain of responsibility pattern and the fold computational framework. The latter is also a visitor pattern in its way. While the backbone computation of our frameworks is implemented with a template method pattern, the command pattern is used to specify the user-defined custom part. Computational frameworks also promote the idea of loose coupling by keeping the computational procedure strictly decoupled from the side effects accumulation.\newline\null
\textbf{\quad Computational frameworks relieve a programmer of some software engineering tasks (i.e software architecture design, writing lower-level system parallelization part) by providing off-the-shelf solutions for some specific scientific computation problems. Examples of these problems can be found in the Section \ref{background_benchmarks_olden}.}
\section{Benchmark studies}
\label{background_benchmarks}
\quad In our projects we used three benchmark suites. We trained our machine learning (ML) based loop parallelization assistant \cite{assistant-aiseps} (see Chapter \ref{assistant}) with Seoul National University's implementation \cite{snu-npb-benchmarks} of the NASA Parallel Benchmarks (NPB) \cite{snu-npb-benchmarks}. The suite comes in two versions: sequential and parallelized with OpenMP. We used the latter to extract ML training labels. Moreover, the suite contains a load of various sorts of parallel loops, which makes it a good training set. For the data-centric parallelization (DCP) project we started with the SPEC CPU2006 benchmarks. The complexity level of that suite and the perspective we derived from the suite study led us to a simpler suite of Olden benchmarks. We used the latter for the project of computational frameworks (see Chapter \ref{frameworks}). Below we provide descriptions of benchmarks, so a reader can develop a better feel for the problems we tackle and the code we work with.
\subsection{NASA Parallel Benchmarks (NPB)}
\label{background_benchmarks_npb}
\quad NAS Parallel Benchmarks (NPB) \cite{nasa-parallel-benchmarks} target performance evaluation of highly parallel supercomputers. NPB are "paper-and-pencil" benchmarks, i.e they do not provide the exact implementation, but rather specify various computational problems at a higher level. There are various implementations. We used the one from Seoul National University (SNU) - SNU NPB \cite{snu-npb-benchmarks}.\newline\null
\quad There are 10 various benchmarks in the suite. Benchmarks perform various scientific computations. They solve various systems of linear equations, compute gradients, work with matrices (compute matrix transpose, inverse, etc.), solve differential equations, solve heat and diffusion equations on the mesh, compute 3d grids, etc. The main data structure used in all SNU NPB benchmarks is a flat multidimensional array. These arrays are processed in loops of various complexity levels including embarrassingly parallel loops like (\textit{a[i] = b[i] + c[i]}), parallel copy and initialization loops (\textit{a[i] = b[i]} and \textit{c[i] = 0.0}), loops computing simple reductions, unrolled loops, multilevel nested loops and loops with uninlined function calls, as well as more complex loops with \textit{if} and \textit{switch} statements and loops with statically unknown iteration numbers and indirect array references (\textit{a[i] = b[c[i]]}).\newline\null
\quad To the biggest part, these benchmarks are inherently parallel, but surprisingly their automatic parallelization with the best state-of-the-art Intel C/C++ Compiler (ICC) \cite{icc-compiler} results into a slowdown (see Section \ref{background_challenges_automatic}). Moreover ICC fails to recognize a lot of parallel loops. Our parallelization assisatant increases parallelization coverage and leads a programmer to achieve a manual expert level parallel benchmark performance faster. 
\subsection{SPEC CPU2006}
\label{background_benchmarks_spec}
\quad The project of Data-Centric Parallelization (DCP) started with the feasibility studies of the SPEC CPU2006 benchmark suite. We studied the feasibility of the automatic data structure recognition techniques on these benchmarks. Although the benchmarks proved to be extremely complex for such techniques, these studies directed our further efforts and ultimately led to the concept of computational frameworks being an inseparable blend of data structures and algorithms. The key lessons we learnt from the SPEC CPU2006 benchmarks are the enormous complexity of the real legacy code and a very close relationship between data structures and algorithms. Let alone automatic techniques, it might take some weeks for an expert engineer to understand what a single benchmark is actually doing. Below we describe some of the benchmarks we looked at.\newline\null
\quad \textbf{429.mcf} \quad The benchmark is derived from MCF, a program used for single-depot vehicle scheduling in public mass transportation. The benchmark operates with a complex network of nodes and arcs linearly allocated on the heap memory. Despite the simplicity of allocation, every node and arc has numerous pointers forming several object linking chains. Pointers are set in different places within the source code base (during allocation as well as during consecutive network structure updates), making the deduction of the actual data structure shape a task of grand complexity. The network forms a spanning tree with several properties true of its nodes: every node has only one child pointer and if a node has several children, then the latter are connected through sibling pointers starting from the first child.\newline\null
\quad\textbf{The tree data structure presents a high interest from the point of its recognition. But even a manual source code analysis and transformation requires a serious effort. Static automatic techniques seem infeasible, while dynamic ones seem to be a grand challenge.}\newline\null
\quad \textbf{456.hmmer} \quad Searches a DNA sequence database given a Profile Hidden Markov Model (HMM). The benchmark uses the Viterbi algorithm. The implementation works with four dynamic programming matrices allocated linearly as arrays. The algorithm walks either horizontally or diagonally along these matrices and computes reductions of maps. The computation is parallelizable and there has been successful work \cite{Ganesan:2010:AHG:1854776.1854844}\cite{inria} doing it manually for specialized hardware.\newline\null
\quad\textbf{Despite the complexity of its core function \textit{P7Viterbi()}, the benchmark presents a very high interest for the application of our computational frameworks. Reductions of maps perfectly fit the purpose. We view it as future work.}\newline\null
\quad \textbf{400.perlbench} \quad This benchmark is a cut-down Perl interpreter, which implements the regular expression matching state machine. The benchmark processes the bitcode of a compiled regular expression instruction by instruction. Although instructions have the same size and are laid out linearly in memory, there might be branches and the whole processing happens in a linked-list offset-directed fashion. That requires sequential execution and is far beyond the capabilities of any existent techniques.\newline\null
\quad\textbf{The benchmark is neither parallel nor a simple one. It makes no point to apply any recognition techniques here.}\newline\null
\quad \textbf{470.lbm} \quad The benchmark implements a Lattice Boltzmann Method (LBM) and is a relatively simple one (around ~1400 LOC). The main underlying data structures the benchmark works with are the two 3D grids mapped onto a linear array space, which simulate incompressible fluids in 3D. The benchmark runs over arrays a specified number of time steps. Every array element represents a point from a 3D grid and consists of a number of velocity vector projections at this point. The values of these projections are being combined and mapped in a stencil fashion. The computation is highly parallel.\newline\null
\quad\textbf{The benchmark operates with 3D grids laid out on regular arrays. The latter do not present a great deal of interest from the point of data structure recognition. Although, it would be interesting to try to recognize an algorithmic stencil.}
\subsection{Olden}
\label{background_benchmarks_olden}
\quad Although the studies of SPEC CPU2006 benchmarks have proved their enormous complexity for the task of automatic data structure recognition, we acquired a good perspective and narrowed our research path to a much simpler suite of Olden benchmarks. Computations and algorithmic patterns present in Olden benchmarks have ultimately led us to the concept of \textit{computational frameworks} (see Chapter \ref{frameworks}).\newline\null
\quad Olden benchmark suite consists of 10 benchmarks. For our project we looked at 6 of those (\textit{bisort}, \textit{health}, \textit{perimeter}, \textit{treeadd}, \textit{mst} and \textit{tsp} benchmarks). The nature of different Olden benchmarks varies. Benchmarks health, treeadd, and perimeter perform essentially the same computational pattern, but for different problems. We call that pattern a fractal and it is basically a parallel tree growth and processing. Benchmarks tsp and mst solve 2 well-known graph problems namely travelling salesman problem (TSP) and minimum spanning tree (MST) construction. The other 4 benchmarks perform scientific numerical computations and are bigger, more complex, and less interesting from the perspective of data structure recognition.\newline\null
\quad \textbf{bisort} \quad \textit{\textbf{Definition} A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with $x_{0} \leq ... \leq x_{k} \geq x_{k+1} \geq ... \geq x_{n} - 1$ for some k, or a circular shift of such a sequence.}\newline\null
\quad The sequence is implemented as a binary tree (recursive calls to the left and right subtrees). The algorithm is based on a sorting comparator network consisting of several layers. The network can be and is implemented in a divide and conquer way similar to that of a well-known merge sort. Sort() function is called on the left and right array halves recursively. The merging step of the merge sort algorithm is substituted with compare-and-swap step. The latter is possible due to input sequences required to be bitonic.\newline\null
\quad\textbf{The benchmark is heavily based on pointers, tree swaps, and rotations. It presents an interest from the point of divide and conquer algorithm recognition. Static techniques are unlikely to handle the legacy source code of that complexity and style. Dynamic techniques might be able to see the binary tree.}\newline\null
\quad \textbf{health} \quad The health benchmark fits into the fractal computational framework the best and hence delivers the most promising performance results. Health benchmark does a simulation of the Columbian healthcare system and is based on a complete 4-ary tree of villages. One can view the tree as the hierarchical structure reflecting various levels of municipal divisions: the root is the capital, leaves represent villages and the nodes on intermediate tree levels represent small towns and bigger cities. The closer to the root, the bigger the settlement and higher the level of hospital expertise. Every village has its own hospital. Figure \ref{fig:health_benchmark} illustrates the tree.
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/village_tree.pdf}
  \captionof{figure}{The \textit{health} benchmark.}
  \label{fig:health_benchmark}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/power_scheme.pdf}
  \captionof{figure}{The \textit{power} benchmark.}
  \label{fig:power_benchmark}
\end{minipage}
\end{figure}\newline\null
\quad Simulation starts at the tree root and goes down to the leaves. Some people fall ill in every village and when they do they go into the local hospital for an assessment and in case they are ill for treatment. If local staff cannot give an accurate diagnosis a patient goes up the tree to a bigger settlement with a higher staff expertise level. As the simulation goes the lists of patients waiting, under assessment, or inside the hospital for treatment grow and the benchmark state becomes bigger. So does the workload. The computation is parallel: all tree node child sub-trees can be simulated independently. Child nodes pass lists of patients to their parents and the latter take them to their local hospitals.\newline\null     
\quad \textbf{The benchmark operates with quad tree structures and does it in a highly parallel fashion. In our work, we call that pattern a fractal. The latter is a computational framework, i.e. the blend of an algorithm and a data structure. The whole structure can be aimed at for automatic recognition (not just a separate tree or an algorithmic skeleton, but both) even with static techniques.}\newline\null
\quad \textbf{perimeter} \quad The perimeter benchmark is another example of the fractal framework. The benchmark computes the perimeter of a ring (r=1024, R=2048). Figure \ref{fig:perimeter_benchmark} illustrates the process. The ring is placed onto a square, which is then being continuously and recursively divided into 4 equal sub-squares (southwest, northwest, southeast, and northeast). Only guided by the stop condition the process continues further. We divide the square further if it falls on the intersection with the ring boundary. If the square falls completely inside the ring or completely outside we stop the division. Division also stops, when the square area becomes less than a preset granularity (or equivalently the depth of the tree). The process can be represented as a growing unbalanced quad tree. Squares are the nodes of the tree. Squares inside the ring (all 4 square corners $(x,y)$ are $r^{2} < x*x + y*y < R^{2}$) are painted black, while those outside are painted white. In other words, the growth stops at black and white tree nodes and continues for those representing squares on the intersection with the ring. Finally, the resulting grid is being traversed to catch all flips of color. When the flip is detected we increment the perimeter counter by the square area adjacent to the flip boundary. The latter results into the final perimeter approximation.\newline\null
\quad \textbf{The benchmark operates with quad tree structures and does it in a highly parallel fashion. This fits into the fractal pattern as well. The only difference from the health benchmark is the tree growth stop condition resulting into an unbalanced tree. Automatic fractal recognition seems harder, but still possible.}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=1.0\textwidth]{images/perimeter_benchmark.png}
\caption{The \textbf{perimeter} benchmark. The computational pattern can be represented by an unbalanced quad tree.}
\label{fig:perimeter_benchmark}
\end{center}
\end{figure}\newline\null
\quad \textbf{power} \quad Another key benchmark for us is the power pricing computation benchmark. The benchmark is based on a composite hierarchical structure of C arrays and pointer-based linked lists of various objects: Root, Laterals, Branches, and Leaves. Figure \ref{fig:power_benchmark} illustrates the data structure. The algorithm is basically composed of folds and reductions. C arrays are reduced and linked lists are folded. The algorithm works recursively and starts with the Root::compute() call. The method accumulates the power demand
Demand(P,Q) from all lists of Laterals and does a reduction to a final Demand(P,Q) value. Accumulation starts with the end of each lateral list. Each lateral accumulates its power demand from all its branches and the latter in turn accumulate their power demand from all their leaves. The leaves call optimize\_node() method, which does a chunk of scientific computations (gradients, vectors, etc.), which compute P and Q. Finally, Root::compute() is called iteratively from a while loop of power\_pricing\_problem(). Iteration continues up until P and Q error becomes less than the required epsilon. Before every simulation step, the benchmark does an injection of coefficients into the structure. Every simulation step leaves computed side effects on the coefficients stored within various objects.\newline\null
\quad \textbf{The power benchmark operates with sequences and linked lists. Sequences are reduced and linked lists are folded. These patterns correspond to and inspire our Fold and Reduce computational frameworks.}\newline\null
\quad \textbf{treeadd} \quad The treeadd benchmark is a very small and simple one. It does a recursive reduction on a pointer-linked binary tree. To create a workload the reduction is done repetitively inside the loop.\newline\null
\quad \textbf{The treeadd benchmark is a straightforward example of the fractal computational framework. Although, the workload is too small for its effective application.}\newline\null
\paragraph{mst} Minimum Spanning Tree (MST) benchmark does an MST weight computation of a complete graph. Computation is approximate and the algorithm looks like it might finish with the incorrect result. Nevertheless, the benchmark can be used as a computational workload. Graph is represented as a linked list of vertices. Every node in the list has a hash table of incident edges. Graph is complete: each vertex is connected to all other vertices in the graph (except itself). Algorithm repeatedly traverses the list of vertices and gradually accumulates the MST weight. On every traversal algorithm picks the node in the list to use as an input for the next traversal. In that sense, there is a cross iteration/traversal dependency. The code below summarises the benchmark.\newline\null
\begin{minipage}[t]{\linewidth}
\begin{lstlisting}[caption={The main algorithm of mst benchmark.},label={lst:mst_code},language=C]
vertex = list;
list = list->next;
while (num_vertices) {
    ret = traverse_vertex_list(vertex, list);
    mst_weight += ret.distance; // accumulate the final result
    vertex = ret.vertex; // next vertex to measure the distance against
    num_vertices--;
}
return mst_weight;
\end{lstlisting}
\end{minipage}
\paragraph{tsp} Travelling Salesman Problem (TSP). The benchmark generates a set of dots scattered on a 2D plane. Dots represent cities and are specified by their (x,y) coordinates. The TSP problem is to visit all the cities and return to the city of origin having passed the minimal distance. In other words, the algorithm returns a cycled sequence of cities, where the proximity of elements in the sequence in terms of order means their spatial proximity on the 2D plane. The algorithmâ€™s work resembles that of an insertion sort. The sequence is divided into 2 parts. Ordered part and unordered part. At first, the ordered part consists of just 1 element. On every iteration, the algorithm takes the next element out of the unordered part, finds it a pairing element inside the ordered part (with the minimal distance between them), and inserts the element into the ordered subsequence next to its pair. In the end, we get the sequence with the property that closest dots stand the closest in the sequence. The benchmark is based on a binary tree being transformed into a doubly-linked list. Every node of the tree represents a city located on a 2D plane with randomly generated (x,y) coordinates. The build\_tree() method is written in a way to generate a uniform distribution of dots on the plane.

%

%