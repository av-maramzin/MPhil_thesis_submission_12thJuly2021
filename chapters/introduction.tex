\chapter{Introduction}
\label{introduction}
\section{Overview}
\label{introduction_overview}
% The importance of software parallelization
\quad Parallelism has become pervasive in the world of computing, with parallel hardware omnipresent across the whole spectrum of various computing systems from low-end embedded devices to high-end supercomputers. Yet, most of the existing software is sequential: be it an old legacy software initially designed for the serial hardware or modern applications developed by application domain experts rather than performance engineers. To exploit all available hardware facilities software has to be parallelized.
\begin{center}
\textbf{\large \textit{The software parallelization problem}}
\end{center}
% Challenges in the field
\begin{description}[style=unboxed,leftmargin=0cm]
\itemsep0em
\item[\textit{Manual parallelization challenges}] The task of software parallelization has characteristically been a very manual process, which is multifaceted, extremely complex, time-consuming, and error-prone. An embarrassingly parallel problem might end up being hidden behind a thoughtless software design, a serial algorithm, or being implemented with unsuccessfully chosen lower-level constructs, such as pointers, heap-allocated and pointer-linked data structures, indirect array referencing, etc. To elegantly and effectively map a parallel problem onto the exact hardware a programmer must work on the various abstract levels and possess expert-level knowledge in various fields from software design and algorithmic patterns in software engineering down to compiler automatic vectorization and hardware cache coherence protocols. It is not always realistic to expect such wide expertise from an average programmer. Done in the wrong way software parallelization can even slow the program down in comparison to its original sequential version.
\item[\textit{Automatic parallelization limitations}] Given the difficulty of manual software parallelization, there have been numerous efforts aimed at automating the task \cite{Bacon:1994:CTH:197405.197406}. For several decades, parallelizing compilers have been the subject of intensive academic research \cite{6813266} and industrial investment (Intel Compiler \cite{icc-compiler}). Yet, for most real-world applications they still fail to deliver parallel performance, and to fully exploit the potential of modern parallel hardware one still needs to apply a significant manual effort. Furthermore, automatic parallelization techniques are limited to narrow domains of straightforward scientific C and Fortran codes and relatively simple computational idioms. When dealing with arbitrary real-world codes automatically parallelizing compilers face a number of problems and challenges. The \textit{Data-Centric Parallelization (DCP) problem} is an important demonstrative example. Listings \ref{lst:introduction_array} and \ref{lst:introduction_list} illustrate the problem and show how easily automatic parallelization\footnote{By parallelizable loop in this context we mean that auto parallelizing compiler can statically prove the non-existence of loop-carried dependencies and generate parallel or vector code.} can be hampered. In an array-based implementation (Listing \ref{lst:introduction_array}) a compiler knows addresses of all sequence elements statically and can generate the code processing different array elements in parallel, while in a linked-list based implementation (Listing \ref{lst:introduction_list}) we see a pointer chasing code, where addresses of sequence elements will be resolved only dynamically and a compiler cannot generate parallel code in advance. In a real world code, the problem is obviously a way more challenging: \textit{data structures are closely entangled with algorithms}. Parallelization of these codes often requires a human mind.\newline\null
\begin{minipage}[t]{0.5\linewidth}
\begin{lstlisting}[caption={\raggedright Parallelizable loop operating on what is clear to compiler a \textbf{linear array}.},label={lst:introduction_array},language=C]
int a[1024];
for (int i=0; i<1024; i++) {
  a[i]=a[i]+1;
}
\end{lstlisting}
\end{minipage}
%
\begin{minipage}[t]{0.5\linewidth}
\begin{lstlisting}[caption={\raggedright Non-parallelizable loop operating on what programmer knows is a \textbf{linked-list}.},label={lst:introduction_list},language=C]
struct Node* nptr;
for (p=nptr; p!=NULL; p=p->next) {
  p->value+=1;
}
\end{lstlisting}
\end{minipage}
\item[\textit{Machine learning based parallelization applicability}] There were attempts to tackle the problem in another way. There is a vast body of research into utilizing more exotic machine learning based methods in the field of software parallelization. A good overview is provided by \cite{ml-oboyle}. These methods have proved to be extremely useful and high performing on some compilation technology problems like selecting the best compiler flags or finding the most optimal compiler optimization parameters (like loop unroll or function inline factors). However, due to the inherent statistical errors and unavailability of large training data sets for compilation problems these methods have not yet found a widespread application in the area of software parallelization.
\end{description}
\begin{center}
\textbf{\large \textit{The assistant solution}}
\end{center}
\quad In this thesis, we are not trying to find a "silver bullet" and solve the problem of automatic parallelization. Neither do we try to tune machine learning algorithms to a perfect 100\% prediction accuracy. Given the difficulty of the obstacles faced by the field today, we do not expect that programmers will be liberated from performing manual parallelization in the near future~\cite{Larsen:2012:PML:2410141.2410600}. Instead, we acknowledge the role of a human programmer in the software parallelization process, but we do not expect the programmer to be an expert either. What we try to do is to \textit{\textbf{reduce the manual effort}} involved in the task by providing a programmer with a parallelization \textit{assistant solution}. Our solution alleviates the task and makes parallelism more accessible to an average programmer. The assistant solution we propose is as multifaceted as the problem itself. To fully exploit all the potential of software parallelization a programmer has to work on several conceptual levels. Thus, the assistant solution consists of a machine learning based loop parallelization tool \cite{assistant-aiseps} aiming at the finest levels of granularity\footnote{In this thesis, by the granularity of parallelizing source code transformation, we mean the scope and complexity of the change. For example, doing array privatization in a loop and adding one OpenMP pragma before it would be a relatively fine-grained parallelizing transformation. This transformation is tiny, does not require the change of algorithm or underlying data structure type. Whereas, identifying a parallel tree reduction in a complex sequential pointer-based code and substituting it with a parallelizable alternative would classify as a coarse-grained parallelization. That transformation would require a lot of effort in identifying the data structure type and proving it is a tree and then substituting the code with a parallelizable alternative. Note, that this terminology might be misleading as it does not always correctly reflect the ultimate parallel speedup: we might have a small, but long-running loop in the code. But most of the time in practice coarse-grained transformations will indeed materialize into more significant performance improvements.}, namely the program loops and a library of computational frameworks \cite{frameworks-repo} aiming at a coarse-grained parallelization on a higher level of software architecture design, algorithm and data structure choice.
\section{Loop Parallelization Assistant}
\label{introduction_assistant}
\quad Despite decades of intensive research in automatic software
parallelization~\cite{6813266}, fully exploiting the potential of modern parallel hardware still requires a significant manual effort. Chapter \ref{assistant} introduces a novel parallelization assistant that aids a programmer in the software parallelization process in the frequent case where automatic approaches fail. The assistant works at the finer levels of granularity, namely the program loops. Loops are compelling candidates for parallelization, as they are naturally decomposable and tend to capture most of the execution time in a program. The assistant reduces the manual effort in this process by presenting a programmer with a ranking of program loops that are most likely to 1) require little or no effort for successful parallelization and 2) improve the program's performance when parallelized. Thus, it improves over the traditional, profile-guided process by also taking into account the \emph{probability} of potential parallelization for each of the profiled loops.

At the core of our parallelization assistant resides a novel machine-learning (ML) model of loop parallelizability. Focusing on loops allows the model to leverage a large number of specific analyses available in modern compilers, such as generalized iterator recognition~\cite{Manilov:2018:GPI:3178372.3179511} and loop dependence analysis~\cite{Jensen:2017:ILD:3132652.3095754}. The model encodes the results of these analyses together with basic properties of the loops as machine learning \textit{features}. The loop parallelizability model is trained, validated, and tested on 1415 loops from the SNU NAS Parallel Benchmarks (SNU NPB)~\cite{Seo:2011:PCN:2357490.2358063}. The loops are labelled using a combination of expert OpenMP~\cite{Dagum:1998:OIA:615255.615542} annotations and optimization reports from the Intel \cpp{} Compiler (ICC), a
production-quality parallelizing compiler. The model is evaluated on multiple machine learning algorithms. The evaluation shows that -- despite the limited size of the data set -- our model achieves a prediction accuracy higher than 90\%.\newline\null
\quad The parallelization assistant combines inference on the parallelizability model with traditional profiling to rank higher those loops with a high probability of being parallelizable and impacting the program performance. An evaluation on eight programs from the SNU NPB suite shows that the program performance tends to improve faster as loops are parallelized in the ranking order suggested by our parallelization assistant compared to a traditional order based on profiling only. On average, following the order suggested by the assistant reduces by approximately 20\% the number of lines of code a programmer has to examine manually to parallelize SNU NPB to its expert-level speedup. Given the high level of effort involved in manual analysis, such a reduction can translate into substantial development cost savings.

\subsection{Contributions of our Loop Parallelization Assistant}
\quad In summary, our machine learning based loop parallelization assistant makes the following contributions:
\begin{itemize}[style=unboxed,leftmargin=0cm]
\itemsep0em
\renewcommand\labelitemi{$\vartriangleright$}
\renewcommand\labelitemii{$\bullet$}
\item We introduce a machine learning model, which can be used to predict the probability with which sequential C loops can be parallelized (Sections~\ref{predicting_parallel_loops} and~\ref{ml_predictive_performance});
\item We integrate profiling of execution time with our novel ML model into a parallelization assistant, which guides the user through a ranked list of loops for parallelization (Section~\ref{practical_applications}); and
\item We demonstrate that our tool and methodology increase programmer productivity by identifying parallel loop candidates better than existing state-of-the-art approaches (Section~\ref{evaluation}).
\end{itemize}

\section{Computational Frameworks library}
\label{introduction_frameworks}
\quad If we have a sequential program and want to improve its performance on a parallel machine, we might use our software parallelization assistant to guide a programmer through a list of ranked application loops and advice on where to concentrate manual efforts. The ultimate software transformation is in the hands of a programmer. A programmer might decide to skip the highlighted loop or parallelize it. The loop might already be parallelizable and one OpenMP pragma would be all that is required, or the loop might require some prior enabling transformation before its parallelization can be accomplished. These source code changes would be classified as relatively fine-grained parallelizing transformations but in some cases, to get the best possible performance results, one might want to make more radical changes, i.e change the algorithm, the data structure, or redesign the software architecture starting from the proposed loop. These tasks often require deep expertise from a programmer and are relatively difficult. A programmer would also face the same challenges if he was to design and develop parallel software from the scratch.\newline\null
\quad To assist a programmer in tackling the problem of parallel software development on a higher level we propose the concept of \textit{computational frameworks} and implement it as a prototype library (see Section \ref{frameworks_library_design}). The concept blends algorithms and data structures to form an elegant higher-level entity that could be used as a parallelization primitive. The concept of computational frameworks has been inspired by the problems in the software parallelization field, a relevant concept of algorithmic skeletons (see the comparative analysis of the two concepts in Section \ref{background_frameworks_vs_skeletons}), and by the complexities of the real-world legacy code. We have already introduced the data-centric parallelization (DCP) problem. Generally, understanding the data structure type requires a thorough understanding of the algorithm that uses it and vise versa. We call it \textit{the problem of data structure and algorithm inseparability}. For many real-world programs, the task of separating data structures from algorithms is extremely challenging, but for some, it does not seem meaningful either. For example, in many Olden benchmarks (see Section \ref{background_benchmarks_olden}) the data structures and algorithms are blended, but the union they form can be framed into an elegant higher-level entity, that can later be parallelized in a nice and structured way. We provide a programmer with a ready-to-use solution to do that - a library of computational frameworks. If a problem to solve fits into computational patterns our frameworks address, then all a programmer needs to do is to get familiar with library API and write the custom part. The software design and its system-level parallelization have already been taken care of.\newline\null 
\quad We demonstrate the utility and potential of the concept by deploying the library on the subset of the Olden benchmark suite. We express benchmark computations in terms of our computational frameworks and rewrite the original legacy C versions of these benchmarks in a modern, better structured, and crucially parallel way. The parallel library version consistently outperforms the sequential version hitting 5-6x speedups on the major benchmarks.\newline\null
\subsection{Contributions of our Computational Frameworks}
\begin{itemize}[style=unboxed,leftmargin=0cm]
\itemsep0em
\renewcommand\labelitemi{$\vartriangleright$}
\renewcommand\labelitemii{$\bullet$}
\item We propose a novel idea of \textit{computational frameworks}, which are higher-level entities that embody both data structures and algorithms; and
\item report on a research prototype C++ template library \cite{frameworks-repo} implementing the idea in a modern, convenient, parallel, and easy to use way;
\item We express computations of some Olden benchmarks in terms of our computational frameworks and rewrite their original sequential legacy C versions with the help of our library in a modern, better structured, portable, and crucially parallel way (see Section \ref{frameworks_main});
\item We demonstrate the potential of the idea and performance of the prototype library on the suite of Olden benchmarks (see Section \ref{frameworks_performance}), achieving consistent parallel speedups of 5-6x on the major benchmarks;
\item Finally, we propose an idea of an alternative software parallelization approach based on our computational frameworks as future work.
\end{itemize}

%